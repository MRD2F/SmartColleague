{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0e4b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import re\n",
    "import os\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.config import Settings\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import pdfplumber\n",
    "import textwrap\n",
    "import warnings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "# Suppress pdfminer warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='pdfminer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef2d25",
   "metadata": {},
   "source": [
    "# General accesses and collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af26dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Load .env file\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"Missing GOOGLE_API_KEY in .env\") \n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e49822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_doc_pdfs=False\n",
    "doc_create_new_collection=False\n",
    "\n",
    "###########################################\n",
    "read_cv_pdfs=False\n",
    "cv_create_new_collection=False\n",
    "cv_text_from_pdf_path = '../data/cvs_from_pdf'\n",
    "cv_pdf_path = '../../curriculum_vitae_data/pdf'\n",
    "\n",
    "#############################################\n",
    "\n",
    "#chroma_client = chromadb.Client()\n",
    "vector_db_path='../data/chromaDB'\n",
    "os.makedirs(vector_db_path, exist_ok=True)\n",
    "chroma_client = chromadb.PersistentClient(path=vector_db_path)\n",
    "\n",
    "doc_collection_name='doc_collection'\n",
    "doc_collection = chroma_client.get_or_create_collection(name=doc_collection_name)\n",
    "\n",
    "cv_collection_name='cv_collection'\n",
    "cv_collection = chroma_client.get_or_create_collection(name=cv_collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f035f290",
   "metadata": {},
   "source": [
    "# General Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1159e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text(use_cropbox=False) + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, max_length=500):\n",
    "    return textwrap.wrap(text, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9285151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_collection(chroma_client, collection_name, delete_collection):\n",
    "    if delete_collection:\n",
    "       chroma_client.delete_collection(name=collection_name)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e188c",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db314567",
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_cv_pdfs:\n",
    "    cv_text = {}\n",
    "    for filename in os.listdir(cv_pdf_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            try:\n",
    "                pdf_path = os.path.join(cv_pdf_path, filename)\n",
    "                text = extract_text_from_pdf(pdf_path)\n",
    "                cv_text[filename.split('.')[0]] = text\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Skipped '{filename}' due to error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fec3fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_cv_pdfs:\n",
    "    df = pd.DataFrame.from_dict(cv_text, orient=\"index\")\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={\"index\": \"ID\", 0: \"resume\"}, inplace=True)\n",
    "    df.to_csv('cv_text_from_pdf.csv', index=False, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ed8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cv_create_new_collection:\n",
    "        df = pd.read_csv(f'{cv_text_from_pdf_path}/cv_text_from_pdf.csv')\n",
    "        df = df[~df.resume.isna()]\n",
    "        df['resume'] = df['resume'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "        #Standardize bullet points to use '•'.\"\"\"\n",
    "        df['resume'] = df['resume'].apply(lambda x: re.sub(r'[·•\\-]+', '•', x))\n",
    "        df[\"name\"] = df['resume'].apply(lambda x: re.search(r\"Name[:\\-]?\\s*(.*)\", x, re.IGNORECASE))\n",
    "\n",
    "        #Make headers consistent and clear\n",
    "        headers = [\n",
    "                \"RANK\", \"SUMMARY\", \"KEY QUALIFICATIONS\", \"HIGHLIGHTS\",\n",
    "                \"ACCOMPLISHMENTS\", \"WORK EXPERIENCE\", \"EDUCATION\", \"SKILLS\", \"NAME\"]\n",
    "        for header in headers:\n",
    "                df['resume'] = df['resume'].apply(lambda x: re.sub(fr'\\s*{header}\\s*', f'\\n\\n{header.title()}:\\n', x, flags=re.IGNORECASE))\n",
    "        #df = pd.read_csv(f'{cvs_path}/cv_text_from_pdf_cleaned.csv')\n",
    "        #df.to_csv('cv_text_from_pdf_cleaned.csv', index=False, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dd11be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cv_create_new_collection:\n",
    "    delete_collection=False\n",
    "    resumes = df.resume.tolist()\n",
    "    ids = df['ID'].apply(lambda x: str(x)).tolist()\n",
    "    names = df['name'].apply(lambda x: str(x)).tolist()\n",
    "\n",
    "    cv_collection.add(\n",
    "        documents=resumes,\n",
    "        metadatas=[{\"ID\": id, \"name\": name} for id, name in zip(ids, names)],\n",
    "        ids=ids,\n",
    "        embeddings=None  # No embeddings needed \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d03ee9",
   "metadata": {},
   "source": [
    "# Company Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bc826d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All PDFs added to the document collection!\n"
     ]
    }
   ],
   "source": [
    "if not read_doc_pdfs and doc_create_new_collection:\n",
    "    df_doc = pd.read_csv('doc_text_from_pdf.csv')\n",
    "    doc_collection_name='doc_collection'\n",
    "  \n",
    "    doc_metadata_names = [\"source\"]\n",
    "    for filename in df_doc.document_name.unique():\n",
    "        text = df_doc[df_doc.document_name == filename].document.values[0]\n",
    "        chunks = chunk_text(text)\n",
    "\n",
    "        print(f\"Processing {filename}...\")\n",
    "        if chunks and filename:\n",
    "            doc_collection.add(\n",
    "                ids=[f\"{filename}_chunk_{i}\" for i in range(len(chunks))],\n",
    "                documents=chunks,\n",
    "                metadatas=[{doc_metadata_names[0]: filename, \"chunk_index\": i} for i in range(len(chunks))]\n",
    "            )\n",
    "        else:\n",
    "            print(\"Skipping add: One or more required lists are empty.\")\n",
    "            print(\"filename: :\", filename)\n",
    "            print(\"chunks: \", chunks)\n",
    "\n",
    "print(\"✅ All PDFs added to the document collection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a0d4d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if read_doc_pdfs and doc_create_new_collection:\n",
    "    documentation_path = '../data/company_documentation/'\n",
    "    doc_collection_name='doc_collection'\n",
    "    delete_collection=False\n",
    "    doc_collection = get_or_create_collection(chroma_client, doc_collection_name, delete_collection)\n",
    "\n",
    "    documentation_text = {}\n",
    "    doc_metadata_names = [\"source\"]\n",
    "    for filename in os.listdir(documentation_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(documentation_path, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            filename = filename.split('.')[0]\n",
    "            documentation_text[filename] = text\n",
    "            chunks = chunk_text(text)\n",
    "\n",
    "            print(f\"Processing {filename}...\")\n",
    "            if chunks and filename:\n",
    "                doc_collection.add(\n",
    "                    ids=[f\"{filename}_chunk_{i}\" for i in range(len(chunks))],\n",
    "                    documents=chunks,\n",
    "                    metadatas=[{doc_metadata_names[0]: filename, \"chunk_index\": i} for i in range(len(chunks))]\n",
    "                )\n",
    "            else:\n",
    "                print(\"Skipping add: One or more required lists are empty.\")\n",
    "                print(\"filename: :\", filename)\n",
    "                print(\"chunks: \", chunks)\n",
    "\n",
    "    print(\"✅ All PDFs added to the document collection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2e9269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df_doc = pd.DataFrame(list(documentation_text.items()), columns=[\"document_name\", \"document\"])\n",
    "    df_doc.to_csv('doc_text_from_pdf.csv', index=False, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e624b77b",
   "metadata": {},
   "source": [
    "# Google GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f02b59",
   "metadata": {},
   "source": [
    "## General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cd256b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rag(collection_name: str, query : str, n_results : int, include : list) -> str:\n",
    "    #Retrieve relevant docs from vector DB\n",
    "    \n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "\n",
    "    retrieved_snippets = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        include=include\n",
    "    )\n",
    "\n",
    "    #Combine retrieved snippets into prompt\n",
    "    context_text = \"\\n\".join(retrieved_snippets[\"documents\"][0])\n",
    "    prompt = f\"Use the following company info to answer the question:\\n{context_text}\\n\\nQuestion: {query}\"\n",
    "\n",
    "    return prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0d8b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(collection_name : str, query : str) -> dict:\n",
    "    \"\"\"Finds the most relevant employee CVs based on the query.\"\"\"\n",
    "    n_results= 10\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    x = results\n",
    "    employees_info = {}\n",
    "    for i in range(len(x['documents'][0])):\n",
    "        info = x['documents'][0][i]\n",
    "        id = x['ids'][0][i]\n",
    "        name = x['metadatas'][0][i]['name']\n",
    "        employees_info[id] = {\n",
    "            'employee_name': name,\n",
    "            'info': info, \n",
    "            'employee_id' : id\n",
    "        }\n",
    "    return employees_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "348865d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_resumes(collection_name : str, query : str) -> dict:\n",
    "    n_results=10\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac2b987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarities(collection_name : str, query : str) -> str:\n",
    "    \"\"\"Finds the most relevant CVs based on the query.\"\"\"\n",
    "    n_results= 10\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    output = f\"Found {len(results['ids'][0])} similar employees for query: '{query}'.\\n\"\n",
    "    output += \"-\" * 80 + \"\\n\"\n",
    "    for i, (doc_id, doc, metadata, distance) in enumerate(zip(\n",
    "        results['ids'][0],\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    )):\n",
    "        similarity_score = (1 - distance) * 100\n",
    "        #output += f\"\\n{i+1}. Recipe Name: {metadata.get('name', 'Unnamed')}\\n\"\n",
    "        output += f\"   Similarity: {similarity_score:.2f}%\\n\"\n",
    "        output += f\"   Employee ID: {doc_id}\\n\"\n",
    "        output += f\"   Employee Name: {metadata.get('name', 'Unnamed')}\\n\"\n",
    "        for key, value in metadata.items():\n",
    "            output += f\"   {key.replace('_', ' ').title()}: {value}\\n\"\n",
    "        output += f\"   resume: {doc}\\n\"  # Include the full document \n",
    "        output += \"-\" * 80 + \"\\n\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca6750b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_employee_info(collection_name : str, employee_id : str) -> dict:\n",
    "    \"\"\"Provides information about a specific employee based on their ID.\"\"\"\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "    employee_id_info =  collection.get(where={\"ID\": employee_id})\n",
    "    employee_id_cv = employee_id_info[\"documents\"][0]\n",
    "    employee_id_name =  employee_id_info[\"metadatas\"][0][\"name\"]\n",
    "    return {'employee_id_cv': employee_id_cv, \n",
    "            'employee_id_name': employee_id_name,\n",
    "            'employee_id': employee_id }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a73555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_similarities(collection_name : str, query : str) -> str:\n",
    "    #results = get_matching_resumes(collection_name, query)\n",
    "\n",
    "    output = find_similarities(collection_name, query)  \n",
    "    print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a444e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_documentation(query : str) -> dict:\n",
    "    n_results=5\n",
    "    collection = chroma_client.get_collection(name=doc_collection_name)\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83ceed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_documentation(query : str) -> dict:\n",
    "    \"\"\"Finds the most relevant company documentation based on the query.\"\"\"\n",
    "    n_results= 10\n",
    "    collection = chroma_client.get_collection(name=doc_collection_name)\n",
    "\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    x = results\n",
    "    docs_info = {}\n",
    "    for i in range(len(x['documents'][0])):\n",
    "        info = x['documents'][0][i]\n",
    "        id = x['metadatas'][0][i]['source']\n",
    "        docs_info[id] = {\n",
    "            'document_content': info, \n",
    "            'document_name' : id\n",
    "        }\n",
    "    return docs_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5f4e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_company_documentation(df_doc):\n",
    "    return df_doc.document_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f67c630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instructions():\n",
    "    instructions = \"\"\"You are a helpful employee of a company named Engage Group, you know all its employees, their CVs and capabilities. You want to help your employees colleghes to conect so they can help each other, improving their comunication, knowleadge about each others and their difference proyects, with the main goal of improving the work enviroment and productivity.\n",
    "        You want to help your employees colleghes getting thought all the company documentation, manuals, instruction for requests and all information regarding HR.\n",
    "    \n",
    "    You have access to special tools that let you:\n",
    "    - Retrieve resume CV of all the employees, stored in the collection named cv_collection.\n",
    "    - Query the resume information to match the request of the user, based on the similarity to the user query.\n",
    "    - Access to all the Engage documentation, stored in the collection named doc_collection\n",
    "    - Access to the collection of the Employees CV (cv_collection) and the company documentation (doc_collection)\n",
    "\n",
    "    ---\n",
    "\n",
    "    Available Tools:\n",
    "    - **get_results(collection_name : str, query : str)**  \n",
    "        Finds the most relevant CVs in the system collection (cv_collection) based on the query, returns a dictionary with the cv_collection results after being quered based on the user query.\n",
    "    - **find_similarities(collection_name : str, query : str)**  \n",
    "        Finds the most relevant CVs in the system collection (cv_collection) based on the query, returns a string with the most relevant employee similarity score and summary of their resume / CV.\n",
    "    - **get_employee_info(collection_name : str, employee_id : str)**  \n",
    "        Provides information about a specific employee based on the employee ID, returns a dictionary with the employee: CV, name and ID.\n",
    "    - **get_matching_documentation(query : str)**\n",
    "        Finds the most relevant company documentation in the system collection based on the query, returns a dictionary with the documentation collection.\n",
    "\n",
    "    ---\n",
    "\n",
    "        How to respond:\n",
    "\n",
    "        1. Identify the user's intent, and brevely summarize the problem at the beginning.\n",
    "        2. Use the **most relevant tool(s)** to get the potential employees which skills match the user query.\n",
    "        3. Use `find_similarities` to provide detail information about the possible matches between the user query and employees skills you found, include a short summary.\n",
    "        4. Use `get_employee_info` to get the CV of the employee ID you found, and include it in the response.\n",
    "        5. Use `get_matching_documentation` only for company related querys, to get the all the documentation information related to the user request, and include it in the response.\n",
    "        6. Present results clearly using Markdown formatting.\n",
    "        7. Include a clear note explaining why you think the results are relevant, and how the experience of the ID can help the user.\n",
    "        8. After presenting the results, Summarize the keywords skills of the empoyee IDs you found\n",
    "\n",
    "        ---\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        - **User:** \"Tell me which empoyee ID can help me with a problem of related to machine learning.\"  \n",
    "        **LLM Calls:**  \n",
    "            1. `find_similarities(collection_name=\"cv_collection\", query=\"Tell me which empoyee ID can help me with a problem of related to machine learning.\")`  \n",
    "        - **User:** \"Tell me all the information of the empoyee ID 11813872.\"  \n",
    "        **LLM Calls:**  \n",
    "            1. `get_employee_info(collection_name=\"cv_collection\", employee_id=\"11813872\")`  \n",
    "        - **User:** \"Explain how its the appraisal cycle for a employee at Engage.\"  \n",
    "        **LLM Calls:**  \n",
    "            1. `get_matching_documentation(query=\"Explain how its the appraisal cycle for a employee at Engage\")`  \n",
    "\n",
    "        ---\n",
    "\n",
    "        Be polite, smart, helpful, and accurate. Don't guess data—use the tools! and remember you are talking like a colleague of the user, so be friendly and helpful.\n",
    "        Regarding technical bureaucratic matters the user may be confused with all the documentation, so be gentle, kind and provide simple steps and explinations. \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    return instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e49225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content(user_query, instructions, tools):\n",
    "    \"\"\"Generates content using the Google GenAI API.\"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-001\",\n",
    "        contents=user_query,\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=[instructions],\n",
    "            tools=tools\n",
    "        ),\n",
    "    )\n",
    "    display(Markdown(response.text))\n",
    "\n",
    "tools = [get_results, find_similarities, get_employee_info, get_matching_documentation] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "413deeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = get_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af078e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is the information I found for Employee ID 2162:\n",
       "\n",
       "*   **Employee ID:** 2162\n",
       "*   **Name:** G S PRASANNA KUMAR\n",
       "*   **Resume:** Includes experience in maintenance, project management, and quality management systems (ISO 9001, OSHAS 18001, EHS14001, TPM, TQM, TS16949). Also has experience in diagnosis & maintenance of equipment.\n",
       "\n",
       "While this employee may not have direct experience with LSTMs, their background in process improvement and quality control might provide a useful perspective on addressing overfitting.\n",
       "\n",
       "In summary, the skills that can be useful from the resume of the employee ID 2162 are:\n",
       "\n",
       "*   Quality Management Systems\n",
       "*   Problem Solving\n",
       "*   Process Optimization\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_rag = False\n",
    "n_results = 5\n",
    "include = [\"documents\", \"metadatas\"]\n",
    "user_query = \"How many employess have skills related to data science and machine learning? Groupby the main skills and the employee ID who match this skills and give me their names, email adress and work experience\" \n",
    "user_query = \"Who can help with overfitting in an LSTM machine learning model?\"\n",
    "#user_query = \"Come prendere ferie? passi\"\n",
    "if use_rag:\n",
    "    colllection_name = cv_collection_name\n",
    "    user_query = compute_rag(cv_collection_name, user_query, n_results, include)\n",
    "\n",
    "generate_content(user_query, instructions, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c4ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
